{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Introduction\n",
    "\n",
    "With reference to Samson Zhang’s code and 3Blue1Brown’s detailed explanation of Neural Networks, we will build a Neural Network from scratch to predict digits using NumPy, Pandas, and Matplotlib.\n",
    "\n",
    "The primary focus of this notebook is to develop a deep understanding of the mathematical foundations behind Neural Networks, particularly through the lens of Linear Algebra and Multivariate Calculus. We will rigorously derive key equations for Forward Propagation, Backpropagation, and Gradient Descent, emphasizing concepts such as matrix operations, partial derivatives, and the chain rule.\n",
    "\n",
    "By the end of this notebook, you will gain a strong intuition for how Neural Networks function beyond just implementation, bridging the gap between theory and practice.\n",
    "\n",
    "Youtube Links\n",
    "- Samson Zhang code: https://www.youtube.com/watch?v=w8yWXqWQYmU&t=1745s\n",
    "- 3Blue1Brown: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi \n",
    "\n",
    "Dataset Used: https://www.kaggle.com/c/digit-recognizer/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\andyl\\\\OneDrive\\\\Documents\\\\Projects\\\\Digit_Recognition'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_directory = 'C:/Users/andyl/OneDrive/Documents/Projects/Digit_Recognition' # change the file_directory accordingly\n",
    "\n",
    "os.chdir(file_directory)\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing Libaries and Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are using is the Kaggle \"Digit Recognizer\" competition dataset. It consists of handwritten digit images, each with a resolution of 28 by 28 pixels.\n",
    "\n",
    "Each of these pixel corresponds to a neuron in the input layer of our neural network. For every neuron in this input layer, it typically represents a concept -- in this case, the brightness or intensity of the respective pixel in the image. The brighter the pixel, the higher the activation value for the corresponding neuron.\n",
    "\n",
    "In image representation in computers, grayscale images are typically represented using 8 bits per pixel. This means the activation value of the neuron in the input layer can have integer values between 0 and 255, where 0 represents black (no brightness) and 255 represents white (maximum brightness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the first image (reshape it to 28x28)\n",
    "image_data = data.drop(columns = ['label'])\n",
    "digit_image = image_data.iloc[0].values.reshape(28, 28)\n",
    "\n",
    "# Plotting the image\n",
    "plt.imshow(digit_image, cmap='gray')\n",
    "plt.title(\"Example Digit\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation is extremely important in neural network for the following reasons:\n",
    "\n",
    "**1. Improved Convergence Speed**\n",
    "- Neural network typically converge faster when the input feature are normalised.If the input values vary widely like 0 to 255 in this case, it makes the optimisation algorithm like gradient descent (will be explained in greater details later) to coverge quickly.\n",
    "\n",
    "- It prevents the weight $w$'s parameters from updating too much in certain layers, leading to instability in training.\n",
    "\n",
    "**2. Helps with Activation Function and the Learning Process**\n",
    "- Normalisation ensures the input to these functions stays within a range where the activation functions (explained in greater details later) behave efficiently\n",
    "\n",
    "- Normalisation also ensure that all the input features contribute equally to the learning process, rather than having a handful of neurons dominating the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "M, N = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:N]\n",
    "X_test = X_test / 255 # normalise the pixel values\n",
    "\n",
    "data_train = data[1000:M].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:N]\n",
    "X_train = X_train / 255 # normalise the pixel values\n",
    "_,M_train = X_train.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Neural Network\n",
    "\n",
    "In the human brain, neurons are the fundamental units responsible for processing and transmitting information.\n",
    "\n",
    "Similarly in a neural network, we have layers of artifical neurons, where each neuron typically represents a concept. For example, in the case of image recognition, it could represent the brightness of a pixel, or in the case of a Large Language Model like ChatGPT, a neuron could represent a specific topic or feature. \n",
    "\n",
    "In this section, we will provide a brief introduction to how neural network operates, from the input layer to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple Neural Network Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron has an activation score $a^{(l)}_{j}$ where $l$ represents the $l^{th}$ layer and $j$ represent the $j^{th}$ neuron in that $l^{th}$ layer. \n",
    "\n",
    "For each neuron in the $l^{th}$ layer, we connect every neuron to each individual neuron in $(l+1)^{th}$ layer. A weight $w^{(l)}_{k,j}$ is then assigned for each connection, where $k$ represent the $k^{th}$ neuron in the $(l+1)^{th}$ layer. \n",
    "\n",
    "Notation can be confusing at first, but it is important to differentiate between each neuron in each layer. Refer to Figure 1 below, which shows all the neurons in the $0^{th}$ layer (the input layer) and a neuron in the $1^{st}$ layer.\n",
    "\n",
    "- <img src=\"Figure_1.png\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Figure 1 as example, to calculate for $a^{(1)}_{1}$, we calcuate the weighted sum of the $a^{(0)}_{j}$ for all $j$ and we add a bias paramter for inactivity $b^{(1)}_{k}$. This value will then be passed through an activation function, such as sigmoid function or a rectified linear units (ReLU) function.\n",
    "\n",
    "- Sigmoid Function: produces a value between 0 and 1.\n",
    "- ReLU: outputs the value directly if it is positive, 0 otherwise. (Refer to Figure 2)\n",
    "\n",
    "- <img src=\"Figure_2.png\" width=\"500\"/>\n",
    "\n",
    "In mathematical notation:\n",
    "\n",
    "$$z^{(1)}_{1} = w^{(0)}_{1,1} \\cdot a^{(0)}_{1} + w^{(0)}_{1,2} \\cdot a^{(0)}_{2} + ... + w^{(0)}_{1,k} \\cdot a^{(0)}_{k} + ... + w^{(0)}_{1,n_{0}} \\cdot a^{(0)}_{n_{0}} + b^{(1)}_{1}$$\n",
    "\n",
    "$$a^{(1)}_{1} = g(z^{(1)}_{1}) \\text{ where } g \\text{ is the activation function.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Forward Propagation: Mathematical Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are many connections between all neurons in the $l^{th}$ and each neuron in $(l+1)^{th}$ layer (refer to Figure 3). For simplicity and organisation, we use matrices to represent these calculations in an organised manner. \n",
    "\n",
    "- <img src=\"Figure_3.png\" width=\"300\"/>\n",
    "\n",
    "Using Figure 3 as example,\n",
    "\n",
    "In mathematical notation:\n",
    "\n",
    "$$\n",
    "g\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "w^{(0)}_{1,1} & w^{(0)}_{1,2} & \\cdots & w^{(0)}_{1,j} & \\cdots & w^{(0)}_{1,n_{0}} \\\\\n",
    "\\\\\n",
    "w^{(0)}_{2,1} & w^{(0)}_{2,2} & \\cdots & w^{(0)}_{2,j} & \\cdots & w^{(0)}_{2,n_{0}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{(0)}_{k,1} & w^{(0)}_{k,2} & \\cdots & w^{(0)}_{k,j} & \\cdots & w^{(0)}_{k,n_{0}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{(0)}_{n_{1},1} & w^{(0)}_{n_{1},2} & \\cdots & w^{(0)}_{n_{1},j} & \\cdots & \\cdots w^{(0)}_{n_{1},n_{0}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a^{(0)}_{1}\\\\\n",
    "\\\\\n",
    "a^{(0)}_{2}\\\\\n",
    "\\vdots \\\\\n",
    "a^{(0)}_{j}\\\\\n",
    "\\vdots \\\\\n",
    "a^{(0)}_{n_{0}}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "+\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "b^{(1)}_{1}\\\\\n",
    "\\\\\n",
    "b^{(1)}_{2}\\\\\n",
    "\\vdots \\\\\n",
    "b^{(1)}_{k}\\\\\n",
    "\\vdots \\\\\n",
    "b^{(1)}_{n_{1}}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "\\end{array}\n",
    "\\right)\n",
    "\n",
    "= g(\\mathbf{W}^{(0)}\\mathbf{a}^{(0)} + \\mathbf{b^{(1)}})\n",
    "\n",
    "= g\n",
    "\\left(\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "z^{(1)}_{1}\\\\\n",
    "\\\\\n",
    "z^{(1)}_{2}\\\\\n",
    "\\vdots \\\\\n",
    "z^{(1)}_{k}\\\\\n",
    "\\vdots \\\\\n",
    "z^{(1)}_{n_{1}}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\right)\n",
    "\n",
    "= g(\\mathbf{z}^{(1)})\n",
    "\n",
    "= \\left[\n",
    "\\begin{matrix}\n",
    "a^{(1)}_{1}\\\\\n",
    "\\\\\n",
    "a^{(1)}_{2}\\\\\n",
    "\\vdots \\\\\n",
    "a^{(1)}_{k}\\\\\n",
    "\\vdots \\\\\n",
    "a^{(1)}_{n_{1}}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "= \\mathbf{a}^{(1)}\n",
    "$$\n",
    "\n",
    "$\\text{where}: \\mathbf{W}^{(0)} \\text{ is an } n_{1} \\times n_{0} \\text{ matrix, } \\mathbf{a}^{(0)} \\text{ is a column vector of size } n_{0} \\text{ , } \\mathbf{b}^{(1)} \\text{ , } \\mathbf{z}^{(1)} \\text{ and } \\mathbf{a}^{(1)}  \\text{ are column vectors of size } n_{1}$\n",
    "\n",
    "This process is known as **Forward Propagation**. This process will continue through to the last layer $L^{th}$ layer (the output layer), which will provide the output we are interested in predicting.\n",
    "\n",
    "In this example, for simplicity, we will be working with 3-layer network (further explained in Section 4). Refer to Figure 4.\n",
    "- The $0^{th}$ layer (input layer) will consist of $n_{0} = 784$ neurons, each representing a pixel. \n",
    "- The $1^{st}$ layer will consist of $n_{1} = 10$ neurons, each potentially representing a certain pattern related to the label numbers\n",
    "- The $2^{nd}$ layer (output layer) will have $n_{2} = 10$ neurons, each representing to a digit from 0 to 9.\n",
    "- <img src=\"Figure_4.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax Function (Used for Classification Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problem, the softmax function, a normalising **activation function**, is applied to the last layer of a neural network. It converts raw activation scores (logits) into a probability distribution function, ensuring that all output values lies between 0 and 1 and sum to 1.\n",
    "\n",
    "The softmax activation function $g$ is given by this formula:\n",
    "\n",
    "$$a^{(L)}_{i} = p_{i} = g(\\mathbf{z}^{(L)})_{i} = \\dfrac{e^{z^{(L)}_{i}}}{\\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}}} \\text{ , notice that } \\sum_{i=1}^{n_{L}}{p_{i}} = 1$$\n",
    "\n",
    "In the case of digit recognition example, we will apply 3-layers neural network (further explained in Section 4), the activation scores in the last layer will be passed through the softmax as such:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "z^{(2)}_{1}\\\\\n",
    "z^{(2)}_{2}\\\\\n",
    "\\vdots \\\\\n",
    "z^{(2)}_{10}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "\\xrightarrow{\\text{Softmax}}\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a^{(2)}_{1}\\\\\n",
    "a^{(2)}_{2}\\\\\n",
    "\\vdots\\\\\n",
    "a^{(2)}_{10}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "=\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "p_{1}\\\\\n",
    "p_{2}\\\\\n",
    "\\vdots\\\\\n",
    "p_{10}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where $p_{i}$ represents the probability that the digit is $i-1$. The digit corresponding to the highest $p_{i}$ will be our final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tuning Process\n",
    "\n",
    "In order to improve the model's performance, we need to optimize the weights and biases parameters of the network. The optimization process involves finding the optimal parameters that minimize the cost function, $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training any machine learning model, we will always try to minimise the cost function $C$.\n",
    "\n",
    "There are 2 types of cost functions, for a training data point $m$: \n",
    "\n",
    "- For regression problem, we use the Squared Error:\n",
    "\n",
    " $$ C_{m}(\\mathbf{a}^{(L)}, \\mathbf{y})= \\sum_{i=1}^{n_{\\scriptscriptstyle L}}{(a^{(L)}_{i,m}-y_{i,m})^2} $$\n",
    "\n",
    "- For classification problem, we use the Cross-Entropy:\n",
    "$$ C_{m}(\\mathbf{a}^{(L)}, \\mathbf{y}) = -\\sum_{i=1}^{n_{L}}{y_{i,m} \\cdot \\log{ \\left( a^{(L)}_{i,m} \\right)}}$$\n",
    "\n",
    "where $\\mathbf{y}$ is a column vector representing the desired activation value for each neuron in the last layer.\n",
    "\n",
    "To compute the overall cost of $M$ training data points, the total cost function $C$, which is the function we are interested in minimising, is given by the average of the individual costs:\n",
    "\n",
    "$$ C(\\mathbf{A}^{(L)}, \\mathbf{Y}) = \\dfrac{1}{M}\\sum_{m=1}^{M} C_{m}(\\mathbf{a}^{(L)}, \\mathbf{y}) $$\n",
    "\n",
    "where $ \\mathbf{A}^{(L)} \\text{ , } \\mathbf{Y} \\in \\mathbb{R}^{n_{L} \\times M} $. The dimension of the matrices and the entries are further illustrated in Section 4, where we apply the concepts to the digit recogniser dataset.\n",
    "\n",
    "Notice that $\\mathbf{a}^{(L)}$ are activation scores derived from the weights and biases parameters in all the previous layers. Therefore the cost function $C$ depends on all the weights $w$ and biases $b$ parameters. \n",
    "\n",
    "We want to fine-tune these parameters by minimising $C$. One conventional method is to take the partial derivatives, for example:\n",
    "\n",
    "$$\\dfrac{\\partial C}{\\partial w^{(l)}_{k,j}} = 0$$\n",
    "\n",
    "However, we will face 2 major problems: \n",
    "\n",
    "1. It is too inefficient to compute all the partial derivatives of all the weights and bias parameters, set them equal to 0, and solve for the optimal parameters.\n",
    "\n",
    "2. The first derivative test doesn't necessarily guarantee that we will find the minimum value of $C$. While using Hessian matrix is theoretically possible, it is computationally very inefficent, especially for high-dimension problems.\n",
    "\n",
    "Hence, Machine Learning Engineers and Data Scientists have come up with an ingenious solution by using a concept in Multivariate Calculus,  **Gradient Descent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gradient Descent and its Basis in Multivariate Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gradient Vector**\n",
    "\n",
    "In Multivariate Calculus, we have a concept called **gradient vector**. To better understand how gradient vector works, we will first visualise it in a 3-dimension space. (Refer to Figure 5)\n",
    "\n",
    "Suppose we have a function $z = f(x,y)$, gradient vector is represented by \"nabla\" or \"del\" of $f$, $\\nabla{f}$, given by this formula.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla{f} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial{f}}{\\partial{x}}\\\\\n",
    "\\\\\n",
    "\\dfrac{\\partial{f}}{\\partial{y}}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Each component of $\\nabla{f}$ represents the rate of change of the function $f(x,y)$ with respective to its corresponding variable. $\\dfrac{\\partial f}{\\partial x}$ tells us how $f$ changes as we move in the $x$-direction in an infinitesimal scale. Similarly, $\\dfrac{\\partial f}{\\partial y}$ tells us how $f$ changes as we move in the $y$-direction in an infinitesimal scale.\n",
    "\n",
    "<img src=\"Figure_5.png\" width=\"300\"/>\n",
    "\n",
    "Image used: https://www.math.net/gradient\n",
    "\n",
    "#### **Directional Derivative**\n",
    "\n",
    "In Multivariate Calculus, another important concept related to the gradient vector is the **directional derivative** in terms of the gradient of $f$. Essentially, directional derivative $D_{\\mathbf{u}}(f)$ measures the rate of change of a function $f(x,y)$ in the direction of a given unit vector $\\mathbf{u}$.\n",
    "\n",
    "The directional derivative is denoted as: \n",
    "\n",
    "$$D_{\\mathbf{u}}(f) = \\nabla{f} \\cdot \\mathbf{u} \\text{ where } \\mathbf{u} \\text{ is a unit vector i.e } \\text{ the magnitude of } \\mathbf{u} \\text{ , } \\lVert \\mathbf{u} \\rVert = 1$$ \n",
    "\n",
    "The directional derivative can be interpreted as the length of the projection of the gradient vector $\\nabla{f}$ onto the direction of the unit vector $\\mathbf{u}$. Note that $D_{\\mathbf{u}}(f)$ is a scalar quantity and $\\nabla{f}$ is a vector quantity. (Refer to Figure 6)\n",
    "\n",
    "Length of projection formula is given by: \n",
    "\n",
    "$$proj_{\\mathbf{u}}(\\nabla{f}) = \\dfrac{\\nabla{f} \\cdot  \\mathbf{u}}{\\lVert \\mathbf{u} \\rVert} = \\nabla{f} \\cdot  \\mathbf{u} \\text{ since } \\lVert \\mathbf{u} \\rVert  = 1$$ \n",
    "\n",
    "<img src=\"Figure_6.png\" width=\"300\"/>\n",
    "\n",
    "\n",
    "In Figure 6, notice we have a right-angled triangle, so we can establish the following relationship:\n",
    "\n",
    "$$ cos(\\theta) = \\dfrac{D_{\\mathbf{u}}(f)}{\\lVert \\nabla{f} \\rVert} \\implies  D_{\\mathbf{u}}(f) = \\lVert \\nabla{f} \\rVert cos(\\theta)$$\n",
    "\n",
    "Since $\\lVert \\nabla{f} \\rVert$ is fixed relative to the point, the rate of change is controlled by $cos(\\theta)$. Given that $ -1 \\leq cos(\\theta) \\leq 1$, the maximum rate of change occurs when $cos(\\theta) = 1$, which corresponds to $\\theta = 0$. This means that the unit vector $\\mathbf{u}$ that results in the maximum rate of change of $f$ is in the direction of the gradient vector $\\nabla{f}$ itself. \n",
    "\n",
    "Therefore, we can conclude that the direction of the gradient vector is the **direction of the steepest ascent**, which indicates the direction in which the function $f$ increases most rapidly. Conversely, moving in the opposite direction, $-\\nabla{f}$, is the **direction of the steepest descent**. This principle forms the foundation of **gradient descent**, a key optimisation technique used to minimise the cost function in the most efficient way.\n",
    "\n",
    "#### **In the context of cost function $C$**\n",
    "\n",
    "Back to the function we are interested to minimise which is the cost function $C$ by finding the optimal weight and bias parameters.\n",
    "\n",
    "$$ \n",
    "\\text{Let }\n",
    "\\mathbf{v} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "w^{(0)}_{1,1}\\\\\n",
    "\\vdots \\\\\n",
    "w^{(0)}_{1,n_{0}}\\\\\n",
    "\\\\\n",
    "b^{(1)}_{1}\\\\\n",
    "\\vdots \\\\\n",
    "b^{(L)}_{n_{L}}\\\\\n",
    "\\end{matrix}\n",
    "\\right] \n",
    "\n",
    "\\text{ and }\n",
    "\n",
    "\\nabla{C} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial{C}}{\\partial{w^{(0)}_{1,1}}}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial{C}}{\\partial{w^{(0)}_{1,n_{0}}}}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial{C}}{\\partial{b^{(1)}_{1}}}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial{C}}{\\partial{b^{(L)}_{n_{L}}}}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$\\mathbf{v}$ represents the vector of all the weight and bias parameters and $\\nabla{C}$ represents the gradient vector of the cost function $C$ with respect to all the weight and bias parameters in the neural network. Since the gradient vector points in the direction of the steepest ascent, we can minimise $C$ by updating the parameters in the opposite direction. Therefore the gradient descent update rule can be written as:\n",
    "\n",
    "$$\\mathbf{v} \\leftarrow \\mathbf{v} - \\alpha\\nabla{C}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, which controls the step size of each update, ensuring that we gradually move towards the optimal parameters rather than overshooting. Selecting an appropriate $\\alpha$ is crucial, too large might cause divergence, while too small could slow down convergence.\n",
    "\n",
    "However, it is important to note that we may not always reach the **global minimum** of the cost function $C$, because the outcome of the gradient descent optimisation process heavily depends on the starting point (the initial values of the weights and biases). \n",
    "\n",
    "Referring to Figure 7 below, The optimization algorithm converge to a **local minimum** if we start from point A, but it could converge to the **global minimum** if we start from point B. This is a common challenge in training deep neural networks, especially in high-dimensional spaces where the cost function has multiple minima. Nevertheless, the gradient descent algorithm remains the most efficient of fine-tuning the parameters in practice.\n",
    "\n",
    "<img src=\"Figure_7.png\" width=\"500\"/>\n",
    "\n",
    "Image used: https://medium.com/analytics-vidhya/journey-of-gradient-descent-from-local-to-global-c851eba3d367"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Backward Propagation for the Simplest Network Structure** \n",
    "\n",
    "To calculate each entry $\\nabla{C}$, we use Backward Propagation, an algorithm that efficiently computes gradients. To fully understand Backward Propagation, we first need to visualise the relationship between the parameters and how they influence the cost function.\n",
    "\n",
    "We start with the simplest layers network, with one neuron in each layer, as shown in Figure 8.\n",
    "\n",
    "<img src=\"Figure_8.png\" width=\"500\"/>\n",
    "\n",
    "First, let's recap the activation score and the notations using Figure 8 as an example:\n",
    "\n",
    "1. The calculation of the weighted sum and the bias of inactivity (before activation):\n",
    "$$z^{(L)}_{1} = w^{(L-1)}_{1,1} a^{(L-1)}_{1} + b^{(L)}_{1} $$\n",
    "\n",
    "2. The activation function $g$ applied to the weighted sum $z$:\n",
    "$$a^{(L)}_{1} = g(z^{(L)}_{1}) $$\n",
    "\n",
    "3. The cost function $C$, assuming we only have 1 training data:\n",
    "- For regression problem:\n",
    "\n",
    "$$ C(a^{(L)}_{1}, y_{1}) = (a^{(L)}_{1}-y_{1})^2 \\implies \\dfrac{\\partial C}{\\partial a^{(L)}_{1}} = 2(a^{(L)}_{1} - y_{1})$$\n",
    "\n",
    "- For classification problem:\n",
    "\n",
    "$$ C({a^{(L)}_{1}}, {y}_{1}) = -{y_{1} \\cdot \\log{ \\left( a^{(L)}_{1} \\right)}} \\implies \\dfrac{\\partial C}{\\partial a^{(L)}_{1}} = -\\dfrac{y_{1}}{a^{(L)}_{1}}$$\n",
    "\n",
    "Note that for equations under points (1) and (2), are structually the same for all layers, the only difference is the indexing.\n",
    "\n",
    "To compute the entries in $\\nabla{C}$, suppose we want to find the partial derivative of $C$ with respect to the weight and bias parameters in the $(L-1)^{th}$ layer, we apply the **Chain Rule**. Notice that:\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial w^{(L-1)}_{1,1}} = \\dfrac{\\partial C}{\\partial a^{(L)}_{1}} \\cdot \\dfrac{\\partial a^{(L)}_{1}}{\\partial z^{(L)}_{1}} \\cdot \\dfrac{\\partial z^{(L)}_{1}}{\\partial w^{(L-1)}_{1,1}} = \\dfrac{\\partial C}{\\partial a^{(L)}_{1}} \\cdot g'(z^{(L)}_{1}) \\cdot a^{(L-1)}_{1}$$\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial b^{(L)}_{1}} = \\dfrac{\\partial C}{\\partial a^{(L)}_{1}} \\cdot \\dfrac{\\partial a^{(L)}_{1}}{\\partial z^{(L)}_{1}} \\cdot \\dfrac{\\partial z^{(L)}_{1}}{\\partial b^{(L)}_{1}} = \\dfrac{\\partial C}{\\partial a^{(L)}_{1}} \\cdot g'(z^{(L)}_{1}) \\cdot 1$$\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}} = \\dfrac{\\partial C}{\\partial a^{(L)}_{1}} \\cdot \\dfrac{\\partial a^{(L)}_{1}}{\\partial z^{(L)}_{1}} \\cdot \\dfrac{\\partial z^{(L)}_{1}}{\\partial a^{(L-1)}_{1}} = \\dfrac{\\partial C}{\\partial a^{(L)}_{1}} \\cdot g'(z^{(L)}_{1}) \\cdot w^{(L-1)}_{1,1} $$\n",
    "\n",
    "The term $\\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}}$ may not be an entry of $\\nabla{C}$, but it plays a crucial role in the **Back Propagation** process for calculating the partial derivative of $C$ with respect to the weights and bias parameters of the previous layer. Suppose now we want to find the partial derivative of $C$ with respect to the weight and bias parameters in the $(L-2)^{th}$ layer. Using the **Chain Rule**,\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial w^{(L-2)}_{1,1}} = \\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}} \\cdot \\dfrac{\\partial a^{(L-1)}_{1}}{\\partial z^{(L-1)}_{1}} \\cdot \\dfrac{\\partial z^{(L-1)}_{1}}{\\partial w^{(L-2)}_{1,1}} = \\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}} \\cdot g'(z^{(L-1)}_{1}) \\cdot a^{(L-2)}_{1}$$\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial b^{(L-1)}_{1}} = \\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}} \\cdot \\dfrac{\\partial a^{(L-1)}_{1}}{\\partial z^{(L-1)}_{1}} \\cdot \\dfrac{\\partial z^{(L-1)}_{1}}{\\partial b^{(L-1)}_{1}} = \\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}} \\cdot g'(z^{(L-1)}_{1}) \\cdot 1$$\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial a^{(L-2)}_{1}} = \\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}} \\cdot \\dfrac{\\partial a^{(L-1)}_{1}}{\\partial z^{(L-1)}_{1}} \\cdot \\dfrac{\\partial z^{(L-1)}_{1}}{\\partial a^{(L-2)}_{1}} = \\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}} \\cdot g'(z^{(L-1)}_{1}) \\cdot w^{(L-2)}_{1,1}$$\n",
    "\n",
    "Notice to find the partial derivative of $C$ with respect to the weight and bias parameters in the $(L-2)^{th}$ layer, all of them needs $\\dfrac{\\partial C}{\\partial a^{(L-1)}_{1}}$. Similarly, we use $\\dfrac{\\partial C}{\\partial a^{(L-2)}_{1}}$ to find the partial derivatives of $C$ with respect to the weight and bias parameters in the $(L-3)^{th}$ layer. \n",
    "\n",
    "This process continues layer by layer, moving backwards through the network until we reach to the $0^{th}$ layer (input layer). \n",
    "\n",
    "So **Forward Propagation** calculates the activations and other values from the input layer to the output layer, **Back Propagation** calculates the gradients for all the parameters from the output layer back to the input layer, using **Chain Rule** to update the weights and bias parameters, ultimately minimising the cost function $C$.\n",
    "\n",
    "\n",
    "#### **Extending the idea to a general case**\n",
    "\n",
    "Where we have multiple neurons in the last layer and $M$ number of training data. For a given training data $m$, the cost function is denoted as $C_{m}$ and the partial derivative of the $C_{m}$ with respect to the parameters for the $(L-1)^{th}$ will be the following:\n",
    "\n",
    "$$ \\dfrac{\\partial C_{m}}{\\partial w^{(L-1)}_{k,j}} = \\dfrac{\\partial C_{m}}{\\partial a^{(L)}_{k}} \\cdot \\dfrac{\\partial a^{(L)}_{k}}{\\partial z^{(L)}_{k}} \\cdot \\dfrac{\\partial z^{(L)}_{k}}{\\partial w^{(L-1)}_{k,j}} = \\dfrac{\\partial C_{m}}{\\partial a^{(L)}_{k}} \\cdot g'(z^{(L)}_{k}) \\cdot a^{(L-1)}_{j}$$\n",
    "\n",
    "$$ \\dfrac{\\partial C_{m}}{\\partial b^{(L)}_{k}} = \\dfrac{\\partial C_{m}}{\\partial a^{(L)}_{k}} \\cdot \\dfrac{\\partial a^{(L)}_{k}}{\\partial z^{(L)}_{k}} \\cdot \\dfrac{\\partial z^{(L)}_{k}}{\\partial b^{(L)}_{k}} = \\dfrac{\\partial C_{m}}{\\partial a^{(L)}_{k}} \\cdot g'(z^{(L)}_{k}) \\cdot 1$$\n",
    "\n",
    "$$ \\dfrac{\\partial C_{m}}{\\partial a^{(L-1)}_{j}} =\\sum_{k=1}^{n_{L}} \\left( \\dfrac{\\partial C_{m}}{\\partial a^{(L)}_{k}} \\cdot \\dfrac{\\partial a^{(L)}_{k}}{\\partial z^{(L)}_{k}} \\cdot \\dfrac{\\partial z^{(L)}_{k}}{\\partial a^{(L-1)}_{j}} \\right) = \\sum_{k=1}^{n_{L}} \\left( \\dfrac{\\partial C_{m}}{\\partial a^{(L)}_{k}} \\cdot g'(z^{(L)}_{k}) \\cdot w^{(L-1)}_{k,j} \\right) $$\n",
    "\n",
    "where $j$ and $k$ represents the index for the neurons in $(L-1)^{th}$ and $L^{th}$ layer respectively.\n",
    "\n",
    "Recall that:\n",
    "\n",
    "$$z^{(L)}_{k} = w^{(L-1)}_{k,1} \\cdot a^{(L-1)}_{1} + w^{(L-1)}_{k,2} \\cdot a^{(L-1)}_{2} + ... + w^{(L-1)}_{k,j} \\cdot a^{(L-1)}_{j} + ... + w^{(L-1)}_{k,n_{(L-1)}} \\cdot a^{(L-1)}_{n_{(L-1)}} + b^{(L)}_{k}$$\n",
    "\n",
    "$$a^{(L)}_{k} = g(z^{(L)}_{k}) \\text{ where } g \\text{ is the activation function.}$$\n",
    "\n",
    "The key difference here is in the 3rd equation, where the parital derivative of $C_{m}$ with respect to the activation $a^{(L-1)}_{j}$ in the $(L-1)^{th}$ layer involves summing over all the neurons in the $L$ layer, as each neuron in the $(L-1)^{th}$ layer contributes to $a^{(L)}_{k}$ for all $1 \\leq k \\leq n_{L}$ in the $L^{th}$ layer. \n",
    "\n",
    "Notice the first 2 equations still remain the same as each weight and bias only directly influences one neuron in layer $L$, meaning no summation is required. **HOWEVER**, depending on the activation function $g$ (e.g Softmax) used, the partial derivative may not be as straightforward, as we will demonstrate in Section 4 later.\n",
    "\n",
    "Finally, if we want to find the partial derivatives for the overall cost function $C$, using the linearity property of partial differentiation:\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial w^{(L-1)}_{k,j}} = \\dfrac{1}{M}\\sum_{m=1}^{M} \\dfrac{\\partial C_{m}}{\\partial w^{(L-1)}_{k,j}} $$\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial b^{(L)}_{k}} = \\dfrac{1}{M}\\sum_{m=1}^{M} \\dfrac{\\partial C_{m}}{\\partial b^{(L)}_{k}} $$\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial a^{(L-1)}_{j}} = \\dfrac{1}{M}\\sum_{m=1}^{M} \\dfrac{\\partial C_{m}}{\\partial a^{(L-1)}_{j}} $$\n",
    "\n",
    "The process of **Forward Propagation** and **Back Propagation** will alternate, and this is how neural network gradually learns to optimise/fine-tune its parameters (weights & bias). So the visualisation of the cycle will look like:\n",
    "\n",
    "***--- Forward Propagation → Cost Calculation → Backpropagation → Update Parameters → Repeat ---***\n",
    "\n",
    "#### **Partial Derivative of the Activation Function $g$** \n",
    "\n",
    "- If function $g$ is the **sigmoid function**, then $g$ and $g'$ are defined as:\n",
    "$$ g(z) = \\dfrac{1}{1 + e^{-z}} \\implies g'(z) = \\dfrac{e^{-z}}{(1 + e^{-z})^2} $$\n",
    "\n",
    "- If function $g$ is the **ReLU function**, then $g$ and $g'$ are defined as:\n",
    "$$ g(z) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } z \\leq 0 \\\\\n",
    "z & \\text{if } z > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies\n",
    "\n",
    "g'(z) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } z \\leq 0 \\\\\n",
    "1 & \\text{if } z > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- If function $g$ is the **Softmax function** (explained in Section 2.4), the derivative will be more complex. In this derivation, we will derive $g$ and $g'$ specifically for the output (last) layer $L$, as the Softmax activation function is typically used for the output layer for classification tasks. $g$ and $g'$ are defined as:\n",
    "\n",
    "$$ a^{(L)}_{i} = p_{i} = g(\\mathbf{z}^{(L)})_{i} = \\dfrac{e^{z^{(L)}_{i}}}{\\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}}} $$ \n",
    "\n",
    "If $i = j$, then using quotient rule:\n",
    "\n",
    "$$ \\dfrac{\\partial g(\\mathbf{z}^{(L)})_{i}}{\\partial z^{(L)}_{i}} = \\dfrac{e^{z^{(L)}_{i}} \\cdot \\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}} - e^{z^{(L)}_{i}} \\cdot e^{z^{(L)}_{i}}}{\\left( \\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}} \\right) ^2} = \\dfrac{e^{z^{(L)}_{i}} \\cdot (\\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}}-e^{z^{(L)}_{i}})}{\\left( \\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}} \\right) ^2} $$ \n",
    "\n",
    "$$ = \\dfrac{e^{z^{(L)}_{i}}}{\\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}}} \\cdot \\left( 1- \\dfrac{e^{z^{(L)}_{i}}}{\\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}}} \\right) $$ \n",
    "\n",
    "$$ = a^{(L)}_{i} \\cdot \\left( 1 - a^{(L)}_{i} \\right )  = p_{i} \\cdot (1 - p_{i})$$\n",
    "\n",
    "If $i \\neq j$, then:\n",
    "\n",
    "$$ \\dfrac{\\partial g(\\mathbf{z}^{(L)})_{i}}{\\partial z^{(L)}_{j}} = -\\dfrac{z^{(L)}_{i}}{\\left( \\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}} \\right) ^2} \\cdot e^{z^{(L)}_{j}} $$ \n",
    "\n",
    "$$ =  -\\dfrac{z^{(L)}_{i}}{\\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}}} \\cdot \\dfrac{z^{(L)}_{j}}{\\sum_{k=1}^{n_{L}}{e^{z^{(L)}_{k}}}} $$ \n",
    "\n",
    "$$ = - a^{(L)}_{i} \\cdot a^{(L)}_{j} = -p_{i} \\cdot p_{j} $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial a^{(L)}_{i}}{\\partial z^{(L)}_{j}} = \n",
    "\\dfrac{\\partial g(\\mathbf{z}^{(L)})_{i}}{\\partial z^{(L)}_{j}} =\n",
    "\\begin{cases}\n",
    "a^{(L)}_{i} \\cdot \\left( 1 - a^{(L)}_{i} \\right ) \\text{ , if } i = j \\\\\n",
    "\\\\\n",
    "- a^{(L)}_{i} \\cdot a^{(L)}_{j} = a^{(L)}_{i} \\cdot (0 - a^{(L)}_{j}) \\text{ , if } i \\neq j\n",
    "\\end{cases}\n",
    "\n",
    "\\implies\n",
    "\n",
    "\\dfrac{\\partial a^{(L)}_{i}}{\\partial z^{(L)}_{j}} = a^{(L)}_{i} \\cdot \\left( \\mathbb{I}(i = j) - a^{(L)}_{j} \\right) \n",
    "\n",
    "$$\n",
    "\n",
    "where:\n",
    "$\n",
    "\\mathbb{I}(i = j) =\n",
    "\\begin{cases}\n",
    "1 \\text{ if } i = j\\\\\n",
    "0 \\text{ if } i \\neq j\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For the digit recognition example, we will be using the ReLU function for the hidden layer, and we will use Softmax function for the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applying the concepts to the Digit Recognition dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Understanding the data and its dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a 3-layer neural network for digit recognition.\n",
    "\n",
    "#### **Input Layer ($0^{th}$ layer)** \n",
    "\n",
    "- The input layer will have $n_{0} = 784$ neurons corresponding to the 784 pixels in each 28 x 28 input image. \n",
    "\n",
    "- For a given training data $m$, we denote the activation scores with a column vector: \n",
    "\n",
    "$$\\mathbf{a}^{(0)}_{m} \\in \\mathbb{R}^{n_{0}}$$ \n",
    "\n",
    "- For all $M$ training data, the activations are stored as a matrix:\n",
    "\n",
    "$$\\mathbf{A}^{(0)} \\in \\mathbb{R}^{n_{0} \\times M}$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{(0)}\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\mathbf{a}^{(0)}_{1} & \\mathbf{a}^{(0)}_{2} & \\cdots & \\mathbf{a}^{(0)}_{m} & \\cdots & \\mathbf{a}^{(0)}_{M}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a^{(0)}_{1,1} & a^{(0)}_{1,2} & \\cdots & a^{(0)}_{1,m} & \\cdots & a^{(0)}_{1,M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^{(0)}_{i,1} & a^{(0)}_{i,2} & \\cdots & a^{(0)}_{i,m} & \\cdots & a^{(0)}_{i,M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^{(0)}_{784,1} & a^{(0)}_{784,2} & \\cdots & a^{(0)}_{784,m} & \\cdots & a^{(0)}_{784,M} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "#### **Hidden Layer ($1^{st}$ layer)** \n",
    "\n",
    "- The hidden layer will have $n_{1} = 10$ neurons with ReLU activation function.\n",
    "\n",
    "- For a given training data $m$, we denote the activation scores with a column vector:\n",
    "\n",
    "$$\\mathbf{a}^{(1)}_{m} \\in \\mathbb{R}^{n_{1}}$$ \n",
    "\n",
    "- For all $M$ training data, the activations are stored as a matrix:\n",
    "\n",
    "$$\\mathbf{A}^{(1)} \\in \\mathbb{R}^{n_{1} \\times M}$$\n",
    "\n",
    "- The explicit form of $\\mathbf{A}^{(1)}$ is similar to $\\mathbf{A}^{(0)}$, the only difference is the number of columns.\n",
    "\n",
    "#### **Output Layer ($2^{nd}$ layer)** \n",
    "\n",
    "- The hidden layer will have $n_{2} = 10$ neurons with Softmax activation function, producing a probability distribution over the 10 digit classes (0-9).\n",
    "\n",
    "- For a given training data $m$, we denote the probability scores with a column vector:\n",
    "\n",
    "$$\\mathbf{a}^{(2)}_{m} \\in \\mathbb{R}^{n_{2}}$$ \n",
    "\n",
    "- For all $M$ training data, the activations are stored as a matrix:\n",
    "\n",
    "$$\\mathbf{A}^{(2)} \\in \\mathbb{R}^{n_{2} \\times M}$$\n",
    "\n",
    "- The explicit form of $\\mathbf{A}^{(2)}$ is similar to $\\mathbf{A}^{(0)}$, the only difference is the number of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Forward Propagation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Layer ($0^{th}$ layer)**\n",
    "\n",
    "- The activation values of the $n_{0} = 784$ neurons in the input layer are stored as a matrix:\n",
    "\n",
    "$$\\mathbf{X} = \\mathbf{A}^{(0)} \\in \\mathbb{R}^{n_{0} \\times M}$$\n",
    "\n",
    "#### **Hidden Layer ($1^{st}$ layer)**\n",
    "\n",
    "- The weighted sum matrix $\\mathbf{Z}^{(1)}$ is denoted: \n",
    "\n",
    "$$ \\mathbf{Z}^{(1)} = \\mathbf{W}^{(0)}\\mathbf{X} + \\mathbf{b}^{(1)} \\mathbf{1}_{M}^{T}$$\n",
    "\n",
    "where $ \\mathbf{Z}^{(1)} \\in \\mathbb{R}^{n_{1} \\times M} \\text{ ; } W^{(0)} \\in \\mathbb{R}^{n_{1} \\times n_{0}} \\text{ ; } \\mathbf{X} \\in \\mathbb{R}^{n_{0} \\times M} \\text{ ; } \\mathbf{b}^{(1)} \\in \\mathbb{R}^{n_{1}}  \\text{ ; } \\mathbf{1}_{M} \\in \\mathbb{R}^{M} \\text{ , } \\mathbf{1}_{M} \\text{ is a column vector of ones (of size M) }$\n",
    "\n",
    "- Note that $\\mathbf{1}_{M}^{T}$ allows the bias vector $\\mathbf{b}^{(1)}$ to be added element-wise to each of column of $\\mathbf{W}^{(0)}\\mathbf{X}$. It means the bias is broadcasted across the columns, so each column of $\\mathbf{Z}^{(1)}$ receive the same bias vector.\n",
    "\n",
    "- Note that in the code, we donn't explicitly have $\\mathbf{1}_{M}^{T}$ in the computation. However, NumPy automatically broadcasts the bias vector $\\mathbf{b}^{(1)}$ to have the same shape as $\\mathbf{W}^{(0)}\\mathbf{X}$. This means that $\\mathbf{b}^{(1)}$ gets automatically repeated for each of the $M$ columns in $\\mathbf{W}^{(0)}\\mathbf{X}$\n",
    "\n",
    "- $ \\mathbf{Z}^{(1)}$ is then being passed through the ReLU activation function to obtain the activation scores for the $1^{st}$ layer:\n",
    "\n",
    "$$ \\mathbf{A}^{(1)} = g_{ReLU}(\\mathbf{Z}^{(1)}) $$\n",
    "\n",
    "where $\\mathbf{A}^{(1)} \\in \\mathbb{R}^{n_{1} \\times M}$\n",
    "\n",
    "- Refer to the Section 2.3 Forward Propagation for the explicit matrix computation for one training data.\n",
    "\n",
    "#### **Output Layer ($2^{nd}$ layer)**\n",
    "\n",
    "- Analogous to $\\mathbf{Z}^{(1)}$, the weighted sum matrix $\\mathbf{Z}^{(2)}$ is denoted: \n",
    "\n",
    "$$ \\mathbf{Z}^{(2)} = \\mathbf{W}^{(1)}\\mathbf{A}^{(1)} + \\mathbf{b}^{(2)} \\mathbf{1}_{M}^{T}$$\n",
    "\n",
    "where $\\mathbf{Z}^{(2)} \\in \\mathbb{R}^{n_{2} \\times M} \\text{ ; } W^{(1)} \\in \\mathbb{R}^{n_{2} \\times n_{1}} \\text{ ; } \\mathbf{A}^{(1)} \\in \\mathbb{R}^{n_{1} \\times M} \\text{ ; } \\mathbf{b}^{(2)} \\in \\mathbb{R}^{n_{2}} \\text{ ; } \\mathbf{1}_{M} \\in \\mathbb{R}^{M} \\text{ , } \\mathbf{1}_{M}\\text{ is a column vector of ones (of size M)}$\n",
    "\n",
    "- $ \\mathbf{Z}^{(2)}$ is then being passed through the Softmax activation function to obtain the probability for the $2^{nd}$ layer:\n",
    "\n",
    "$$ \\mathbf{A}^{(2)} = g_{softmax}(\\mathbf{Z}^{(2)}) $$\n",
    "\n",
    "where $\\mathbf{A}^{(2)} \\in \\mathbb{R}^{n_{2} \\times M}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "# a function to randomise the weights and the biases parameters\n",
    "def init_params():\n",
    "    W0 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W1 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W0, b1, W1, b2\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "def forward_prop(W0, b1, W1, b2, X):\n",
    "    Z1 = W0.dot(X) + b1 # Note in Python, even though W0.dot(X) and b1 have different dimension, b1 has the same number of columns as W0.dot(X). NumPy uses broadcasting to align them for the addition and the bias vector b1 gets automatically repeated for each of the M columns in W0.dot(X)\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W1.dot(A1) + b2 # Same explanation as Z1\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Backward Propagation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preliminary: Indexing and some function**\n",
    "\n",
    "$i \\rightarrow$ dummy index variable\n",
    "\n",
    "$j \\rightarrow$ Index for neurons $1^{st}$ (Hidden) Layer\n",
    "\n",
    "$k \\rightarrow$ Index for neurons $2^{nd}$ (Output) Layer\n",
    "\n",
    "$\\mathbb{I} \\rightarrow$ is an indicator function, 1 if True, 0 otherwise\n",
    "\n",
    "\n",
    "#### **1. Output Layer ($2^{nd}$ layer)**\n",
    "\n",
    "##### $ \\text{ 1a) Matrix } d\\mathbf{Z}^{(2)}$\n",
    "\n",
    "General entry for $d\\mathbf{Z}^{(2)}$ is defined as:\n",
    "\n",
    "$$ \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}} \n",
    "= \\sum_{i = 1}^{10}\\left( \\dfrac{\\partial C_{m}}{\\partial a^{(2)}_{i}} \\cdot \\dfrac{\\partial a^{(2)}_{i}}{\\partial z^{(2)}_{k}} \\right)\n",
    "= \\sum_{i = 1}^{10}\\left( -\\dfrac{y_{i}}{a^{(2)}_{i}} \\cdot a^{(2)}_{i} \\cdot \\left[\\mathbb{I}(i=k) - a^{(2)}_{k} \\right] \\right) $$\n",
    "\n",
    "$$ = \\sum_{i = 1}^{10} \\left( y_{i} \\cdot a^{(2)}_{k} \\right) -\\sum_{i = 1}^{10} \\left( y_{i} \\cdot \\mathbb{I}(i = k) \\right) = a^{(2)}_{k} \\cdot \\sum_{i = 1}^{10}(y_{i}) - y_{k} $$\n",
    "\n",
    "$$ = a^{(2)}_{k} - y_{k} $$\n",
    "\n",
    "Note that $\\sum_{i = 1}^{10}(y_{i}) = 1$. Since $\\mathbf{y}$ is a column vector of desired output, which is 1 for the correct label prediction, 0 for the rest. \n",
    "\n",
    "**WARNING**: The activation function $g$ used is the Softmax. Due to the property of Softmax, the parital derivative of $C_{m}$ with respect to the activation $z^{(2)}_{k}$ involves summing over all the neurons in the $2^{nd}$ layer, as $z^{(2)}_{k}$ contributes to all the $a^{(2)}_{i}$ for all $1 \\leq i \\leq n_{2}$ in the $2^{nd}$ layer.\n",
    "\n",
    "In matrix form: \n",
    "\n",
    "$$ \n",
    "d\\mathbf{Z}^{(2)} = \\mathbf{A}^{(2)} - \\mathbf{Y}\n",
    "$$\n",
    "\n",
    "where $ d\\mathbf{Z}^{(2)} \\text{ , } \\mathbf{A}^{(2)} \\text{ , } \\mathbf{Y} \\in \\mathbb{R}^{n_{2} \\times M}$ ($n_{2} = 10$ in this digit recognition example and $M$ is the number of training data.)\n",
    "\n",
    "In explicit form: \n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{1}} & \\dfrac{\\partial C_{2}}{\\partial z^{(2)}_{1}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{1}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{1}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{k}} & \\dfrac{\\partial C_{2}}{\\partial z^{(2)}_{k}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{k}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{10}} & \\dfrac{\\partial C_{2}}{\\partial z^{(2)}_{10}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{10}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{10}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a^{(2)}_{1,1} & a^{(2)}_{1,2} & \\cdots & a^{(2)}_{1,m} & \\cdots & a^{(2)}_{1,M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^{(2)}_{k,1} & a^{(2)}_{k,2} & \\cdots & a^{(2)}_{k,m} & \\cdots & a^{(2)}_{k,M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^{(2)}_{10,1} & a^{(2)}_{10,2} & \\cdots & a^{(2)}_{10,m} & \\cdots & a^{(2)}_{10,M} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "-\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "y_{1,1} & y_{1,2} & \\cdots & y_{1,m} & \\cdots & y_{1,M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y_{k,1} & y_{k,2} & \\cdots & y_{k,m} & \\cdots & y_{k,M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y_{10,1} & y_{10,2} & \\cdots & y_{10,m} & \\cdots & y_{10,M} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "##### $ \\text{ 1b) Matrix } d\\mathbf{b}^{(2)}$\n",
    "\n",
    "General entry for $d\\mathbf{b}^{(2)}$ is defined as:\n",
    "\n",
    "$$ \\dfrac{\\partial C_{m}}{\\partial b^{(2)}_{k}} \n",
    "= \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}} \\cdot \\dfrac{\\partial z^{(2)}_{k}}{\\partial b^{(2)}_{k}}\n",
    "= \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}} \\cdot 1 \n",
    "= \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}} \n",
    "$$\n",
    "\n",
    "**WARNING**: Previously, in Section 3.3 Back Propagation, the formula for $\\dfrac{\\partial C}{\\partial b^{(L)}_{1}}$ is different. Again, this is due to the fact that the activation function used is Softmax in the $2^{nd}$ layer. As a result, we **cannot** use the formula for $\\dfrac{\\partial C}{\\partial b^{(L)}_{1}}$ shown in Section 3.3 Back Propagation directly, and the Chain Rule in partial differentiation becomes more intricate, similarly to the complexity encountered with the partial derivatives for matrix $d\\mathbf{Z}^{(2)}$.\n",
    "\n",
    "We proceed to find the partial derivative of the overall cost function $C$ with respect to the bias $b^{(2)}_{k}$:\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial b^{(2)}_{k}} \n",
    "= \\dfrac{1}{M}\\sum_{m=1}^{M} \\dfrac{\\partial C_{m}}{\\partial b^{(2)}_{k}}  \n",
    "= \\dfrac{1}{M}\\sum_{m=1}^{M} \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}}\n",
    "$$\n",
    "\n",
    "In matrix form: \n",
    "\n",
    "$$ d\\mathbf{b}^{(2)} = \\dfrac{1}{M} \\cdot d\\mathbf{Z}^{(2)} \\mathbf{1}_{M} $$\n",
    "\n",
    "where $ d\\mathbf{b}^{(2)} \\in \\mathbb{R}^{n_{2}} \\text{ ; } d\\mathbf{Z}^{(2)} \\in \\mathbb{R}^{n_{2} \\times M} \\text{ ; } \\mathbf{1}_{M} \\in \\mathbb{R}^{M} \\text{ , } \\mathbf{1}_{M} \\text{ is a column vector of ones (of size M) }$ ($n_{2} = 10$ in this digit recognition example and $M$ is the size of training data.)\n",
    "\n",
    "In explicit form:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial C}{\\partial b^{(2)}_{1}} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial C}{\\partial b^{(2)}_{k}} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial C}{\\partial b^{(2)}_{10}}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "= \\dfrac{1}{M} \\cdot\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{1}} & \\dfrac{\\partial C_{2}}{\\partial z^{(2)}_{1}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{1}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{1}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{k}} & \\dfrac{\\partial C_{2}}{\\partial z^{(2)}_{k}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{k}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{10}} & \\dfrac{\\partial C_{2}}{\\partial z^{(2)}_{10}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{10}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{10}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "$$\n",
    "\n",
    "#### **2. Hidden Layer ($1^{st}$ layer)**\n",
    "\n",
    "##### $\\text{ 2a) Matrix } d\\mathbf{W}^{(1)}$\n",
    "\n",
    "General entry for $d\\mathbf{W}^{(1)}$ is defined as:\n",
    "\n",
    "$$ \\dfrac{\\partial C_{m}}{\\partial w^{(1)}_{k,j}} \n",
    "= \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}}  \\cdot \\dfrac{\\partial z^{(2)}_{k}}{\\partial w^{(1)}_{k,j}} \n",
    "= \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}} \\cdot a^{(1)}_{j} \n",
    "$$\n",
    "\n",
    "**WARNING**: Same reason as the previous warning, we **cannot** use the formula for $\\dfrac{\\partial C_{m}}{\\partial w^{(1)}_{k,j}}$ shown in Section 3.3 Back Propagation directly.\n",
    "\n",
    "We proceed to find the partial derivative of the overall cost function $C$ with respect to the bias $w^{(1)}_{k,j}$:\n",
    "\n",
    "$$ \\dfrac{\\partial C}{\\partial w^{(1)}_{k,j}} = \\dfrac{1}{M}\\sum_{m=1}^{M} \\dfrac{\\partial C_{m}}{\\partial w^{(1)}_{k,j}} \n",
    "\n",
    "= \\dfrac{1}{M}\\sum_{m=1}^{M} \\left( \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}} \\cdot a^{(1)}_{j,m} \\right)$$\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$$ \n",
    "d\\mathbf{W}^{(1)} = \\dfrac{1}{M} \\cdot d\\mathbf{Z}^{(2)} \\left[ \\mathbf{A}^{(1)} \\right]^{T} \n",
    "$$\n",
    "\n",
    "where $d\\mathbf{W}^{(1)} \\in \\mathbb{R}^{n_{2} \\times n_{1}} \\text{ , } d\\mathbf{Z}^{(2)} \\in \\mathbb{R}^{n_{2} \\times M} \\text{ , } \\left[ \\mathbf{A}^{(1)} \\right]^{T} \\in \\mathbb{R}^{M \\times n_{1}}$ ($n_{1} = 10 \\text{ , } n_{2} = 10$ in this digit recognition example and $M$ is the size of training data.)\n",
    "\n",
    "In explicit form:\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial C}{\\partial w^{(1)}_{1,1}} & \\cdots & \\dfrac{\\partial C}{\\partial w^{(1)}_{1,j}} & \\cdots & \\dfrac{\\partial C}{\\partial w^{(1)}_{1,10}} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C}{\\partial w^{(1)}_{k,1}} & \\ \\cdots & \\dfrac{\\partial C}{\\partial w^{(1)}_{k,j}} & \\cdots & \\dfrac{\\partial C}{\\partial w^{(1)}_{k,10}} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C}{\\partial w^{(1)}_{10,1}} & \\cdots & \\dfrac{\\partial C}{\\partial w^{(1)}_{10,j}} & \\cdots & \\dfrac{\\partial C}{\\partial w^{(1)}_{10,10}}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "=\n",
    "\\dfrac{1}{M} \\cdot\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{1}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{1}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{1}} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{k}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{k}} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{10}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{10}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{10}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a^{(1)}_{1,1} & \\cdots & a^{(1)}_{1,m} & \\cdots & a^{(1)}_{10,1} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^{(1)}_{1,m} & \\cdots & a^{(1)}_{j,m} & \\cdots & a^{(1)}_{10,m} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^{(1)}_{1,M} & \\cdots & a^{(1)}_{j,M} & \\cdots & a^{(1)}_{10,M} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "##### $\\text{ 2b) Matrix } d\\mathbf{Z}^{(1)}$\n",
    "\n",
    "General entry for $d\\mathbf{Z}^{(1)}$ is defined as:\n",
    "\n",
    "$$\\dfrac{\\partial C_{m}}{\\partial z^{(1)}_{j}} \n",
    "= \\sum_{i=1}^{n_{1}} \\left( \\dfrac{\\partial C_{m}}{\\partial a^{(1)}_{i}} \\cdot \\dfrac{\\partial a^{(1)}_{i}}{\\partial z^{(1)}_{j}} \\right)\n",
    "$$\n",
    "\n",
    "First, we find:\n",
    "\n",
    "$$\\dfrac{\\partial C_{m}}{\\partial a^{(1)}_{j}} \n",
    "= \\sum_{i=1}^{n_{2}} \\left( \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{i}} \\cdot \\dfrac{\\partial z^{(2)}_{i}}{\\partial a^{(1)}_{j}} \\right)\n",
    "= \\sum_{i=1}^{n_{2}} \\left( \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{i}} \\cdot w^{(1)}_{i,j} \\right)\n",
    "= \\sum_{i=1}^{n_{2}} \\left( w^{(1)}_{i,j} \\cdot \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{i}} \\right)\n",
    "$$\n",
    "\n",
    "which is the $(j,m)$ entry of the matrix $\\left[\\mathbf{W}^{(1)} \\right]^{T} d\\mathbf{Z}^{(2)}$\n",
    "\n",
    "Now, the activation function $g$ in the $1^{st}$ layer is ReLU function. Therefore,\n",
    "\n",
    "$$\\dfrac{\\partial C_{m}}{\\partial z^{(1)}_{j}} \n",
    "= \\dfrac{\\partial C_{m}}{\\partial a^{(1)}_{j}} \\cdot \\dfrac{\\partial a^{(1)}_{j}}{\\partial z^{(1)}_{j}}\n",
    "= \\dfrac{\\partial C_{m}}{\\partial a^{(1)}_{j}} \\cdot g'(z^{(1)}_{j})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\dfrac{\\partial C_{m}}{\\partial a^{(1)}_{j}} \\cdot \\mathbb{I}(z^{(1)}_{j} > 0)\n",
    "$$\n",
    "\n",
    "**WARNING**: Note that, because the activation function is ReLU function, $z^{(1)}_{j}$ only contributes to $a^{(1)}_{j}$. Therefore, in this case, the chain rule for partial derivatives holds here.\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$$d\\mathbf{Z}^{(1)} = \\left[ \\mathbf{W}^{(1)} \\right]^{T} d\\mathbf{Z}^{(2)} \\odot g'_{ReLU}(\\mathbf{Z}^{(1)})$$\n",
    "\n",
    "where $d\\mathbf{Z}^{(1)} \\in \\mathbb{R}^{n_{1} \\times M} \\text{ ; } \\left[ \\mathbf{W}^{(1)} \\right]^{T} \\in \\mathbb{R}^{n_{1} \\times n_{2}} \\text{ ; } d\\mathbf{Z}^{(2)} \\in \\mathbb{R}^{n_{2} \\times M} \\text{ ; } g'_{ReLU}(\\mathbf{Z}^{(1)}) \\in \\mathbb{R}^{n_{1} \\times M}$ ($n_{1} = 10 \\text{ , } n_{2} = 10$ in this digit recognition example and $M$ is the size of training data.) \n",
    "\n",
    "Note $\\odot$ represents Hadamard product which multiplies corresponding elements.\n",
    "\n",
    "In explicit form:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(1)}_{1}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(1)}_{1}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(1)}_{1}} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(1)}_{j}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(1)}_{j}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(1)}_{j}} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(1)}_{10}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(1)}_{10}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(1)}_{10}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \n",
    "\\left(\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "w^{(1)}_{1,1} & \\cdots & w^{(1)}_{1,k} & \\cdots & w^{(0)}_{1,10} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{(1)}_{j,1} & \\cdots & w^{(0)}_{j,k} & \\cdots & w^{(0)}_{j,10} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w^{(0)}_{10,1} & \\cdots & w^{(0)}_{10,k} & \\cdots & \\cdots w^{(0)}_{10,10} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{1}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{1}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{1}} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{k}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{k}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{k}} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial C_{1}}{\\partial z^{(2)}_{10}} & \\cdots & \\dfrac{\\partial C_{m}}{\\partial z^{(2)}_{10}}& \\cdots & \\dfrac{\\partial C_{M}}{\\partial z^{(2)}_{10}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "\\right)\n",
    "\n",
    "\\odot\n",
    "\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\mathbb{I}(z^{(1)}_{1,1} > 0) & \\cdots & \\mathbb{I}(z^{(1)}_{1,m} > 0) & \\cdots & \\mathbb{I}(z^{(1)}_{1,M} > 0) \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbb{I}(z^{(1)}_{j,1} > 0) & \\cdots & \\mathbb{I}(z^{(1)}_{j,m} > 0) & \\cdots & \\mathbb{I}(z^{(1)}_{j,M} > 0) \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbb{I}(z^{(1)}_{10,1} > 0) & \\cdots & \\mathbb{I}(z^{(1)}_{10,m} > 0) & \\cdots & \\mathbb{I}(z^{(1)}_{10,M} > 0) \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "$$\n",
    "\n",
    "##### $\\text{ 2c) Matrix } d\\mathbf{b}^{(1)}$\n",
    "\n",
    "Analogous to $d\\mathbf{b}^{(2)}$, the matrix form is defined as:\n",
    "\n",
    "$$ d\\mathbf{b}^{(1)} = \\dfrac{1}{M} \\cdot d\\mathbf{Z}^{(1)} \\mathbf{1}_{M} $$\n",
    "\n",
    "where $  d\\mathbf{b}^{(1)} \\in \\mathbb{R}^{n_{1}} \\text{ ; } d\\mathbf{Z}^{(1)} \\in \\mathbb{R}^{n_{1} \\times M} \\text{ ; } \\mathbf{1}_{M} \\in \\mathbb{R}^{M} \\text{ , } \\mathbf{1}_{M} \\text{ is a column vector of ones (of size M) }$ ($n_{1} = 10$ in this digit recognition example and $M$ is the size of training data.)\n",
    "\n",
    "#### **3. Input Layer ($0^{th}$ layer)**\n",
    "\n",
    "- $\\text{ 3a) Matrix } d\\mathbf{W}^{(0)}$\n",
    "\n",
    "Analogous to $d\\mathbf{W}^{(1)}$, the matrix form is defined as:\n",
    "\n",
    "$$ d\\mathbf{W}^{(0)} = \\dfrac{1}{M} \\cdot d\\mathbf{Z}^{(1)} \\left[ \\mathbf{A}^{(0)} \\right]^{T} $$\n",
    "\n",
    "where $d\\mathbf{W}^{(0)} \\in \\mathbb{R}^{n_{1} \\times n_{0}} \\text{ ; } d\\mathbf{Z}^{(1)} \\in \\mathbb{R}^{n_{1} \\times M} \\text{ ; } \\left[ \\mathbf{A}^{(0)} \\right]^{T} \\in \\mathbb{R}^{M \\times n_{0}}$ ($n_{0} = 784 \\text{ , } n_{1} = 10$ in this digit recognition example and $M$ is the size of training data.)\n",
    "\n",
    "#### **4. Updating the parameters**\n",
    "\n",
    "Using the idea of Gradient Descent (explained in Section 3.2), we can iteratively fine-tune the parameters to ultimately reduce the overall cost function $C$\n",
    "\n",
    "$$ \\mathbf{b}^{(2)} \\leftarrow \\mathbf{b}^{(2)} - \\alpha \\cdot d\\mathbf{b}^{(2)} $$\n",
    "$$ \\mathbf{W}^{(1)} \\leftarrow \\mathbf{W}^{(1)} - \\alpha \\cdot d\\mathbf{W}^{(1)} $$\n",
    "$$ \\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\alpha \\cdot d\\mathbf{b}^{(1)} $$\n",
    "$$ \\mathbf{W}^{(0)} \\leftarrow \\mathbf{W}^{(0)} - \\alpha \\cdot d\\mathbf{W}^{(0)} $$\n",
    "\n",
    "where $\\alpha$ represents the step size or the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentially produces 1 if True (Z > 0), 0 otherwise\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W0, W1, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW1 = 1 / M * dZ2.dot(A1.T)\n",
    "    db2 = 1 / M * np.sum(dZ2, axis = 1, keepdims = True) # axis = 1 means adding the elements by row, keepdims ensure that it is a 2D array with dimension of (10,1)\n",
    "    dZ1 = W1.T.dot(dZ2) * ReLU_deriv(Z1) # * in Python is the Hadamard product\n",
    "    dW0 = 1 / M * dZ1.dot(X.T)\n",
    "    db1 = 1 / M * np.sum(dZ1, axis = 1, keepdims = True) # Same explanation as db2\n",
    "    return dW0, db1, dW1, db2\n",
    "\n",
    "def update_params(W0, b1, W1, b2, dW0, db1, dW1, db2, alpha):\n",
    "    W0 = W0 - alpha * dW0\n",
    "    b1 = b1 - alpha * db1  \n",
    "    W1 = W1 - alpha * dW1  \n",
    "    b2 = b2 - alpha * db2  \n",
    "    return W0, b1, W1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(Y_train) # shows the probability of the desired outputs Y, which is 1 for the correct label, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the highest probability from a probability distribution function that was obtained after passing the activation scores of the last layers.\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "# The proportion of the model correctly identifying the label, out of the total number of training data\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W0, b1, W1, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W0, b1, W1, b2, X)\n",
    "        dW0, db1, dW1, db2 = backward_prop(Z1, A1, Z2, A2, W0, W1, X, Y)\n",
    "        W0, b1, W1, b2 = update_params(W0, b1, W1, b2, dW0, db1, dW1, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(\"Accuracy: \", get_accuracy(predictions, Y))\n",
    "    return W0, b1, W1, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Applying the code on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[6 6 6 ... 6 6 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.08419512195121952\n",
      "Iteration:  10\n",
      "[2 0 0 ... 0 2 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.12841463414634147\n",
      "Iteration:  20\n",
      "[4 0 6 ... 6 0 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.20629268292682926\n",
      "Iteration:  30\n",
      "[4 6 6 ... 6 0 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.30148780487804877\n",
      "Iteration:  40\n",
      "[4 6 6 ... 6 0 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.38058536585365854\n",
      "Iteration:  50\n",
      "[8 6 6 ... 6 0 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.4438780487804878\n",
      "Iteration:  60\n",
      "[8 6 6 ... 6 0 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.4948780487804878\n",
      "Iteration:  70\n",
      "[8 6 6 ... 6 0 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.5470731707317074\n",
      "Iteration:  80\n",
      "[8 6 6 ... 6 0 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.5868536585365853\n",
      "Iteration:  90\n",
      "[1 9 6 ... 6 9 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.6183414634146341\n",
      "Iteration:  100\n",
      "[1 9 6 ... 6 9 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.6448292682926829\n",
      "Iteration:  110\n",
      "[1 9 6 ... 6 9 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.6663414634146342\n",
      "Iteration:  120\n",
      "[1 9 6 ... 6 9 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.6856585365853659\n",
      "Iteration:  130\n",
      "[1 9 6 ... 6 9 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7009024390243902\n",
      "Iteration:  140\n",
      "[3 9 6 ... 6 9 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7147073170731707\n",
      "Iteration:  150\n",
      "[3 9 6 ... 6 9 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7261463414634146\n",
      "Iteration:  160\n",
      "[3 9 6 ... 6 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7367073170731707\n",
      "Iteration:  170\n",
      "[3 9 6 ... 6 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.747\n",
      "Iteration:  180\n",
      "[3 9 6 ... 6 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7560487804878049\n",
      "Iteration:  190\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7638780487804878\n",
      "Iteration:  200\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7713414634146342\n",
      "Iteration:  210\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7778780487804878\n",
      "Iteration:  220\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7829268292682927\n",
      "Iteration:  230\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7873414634146342\n",
      "Iteration:  240\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7924878048780488\n",
      "Iteration:  250\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.7976341463414635\n",
      "Iteration:  260\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.802\n",
      "Iteration:  270\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8053414634146342\n",
      "Iteration:  280\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.809439024390244\n",
      "Iteration:  290\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8119024390243903\n",
      "Iteration:  300\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8152195121951219\n",
      "Iteration:  310\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8184634146341463\n",
      "Iteration:  320\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8215853658536585\n",
      "Iteration:  330\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8241951219512195\n",
      "Iteration:  340\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8271219512195122\n",
      "Iteration:  350\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8296097560975609\n",
      "Iteration:  360\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8313170731707317\n",
      "Iteration:  370\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8333170731707317\n",
      "Iteration:  380\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8360487804878048\n",
      "Iteration:  390\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8381463414634146\n",
      "Iteration:  400\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8402195121951219\n",
      "Iteration:  410\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8419024390243902\n",
      "Iteration:  420\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8435609756097561\n",
      "Iteration:  430\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8450731707317073\n",
      "Iteration:  440\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8468048780487805\n",
      "Iteration:  450\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8481463414634146\n",
      "Iteration:  460\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8498780487804878\n",
      "Iteration:  470\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8511463414634146\n",
      "Iteration:  480\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8523170731707317\n",
      "Iteration:  490\n",
      "[3 9 6 ... 2 5 6] [3 9 6 ... 2 5 6]\n",
      "Accuracy:  0.8534390243902439\n"
     ]
    }
   ],
   "source": [
    "W0, b1, W1, b2 = gradient_descent(X_train, Y_train, 0.1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W0, b1, W1, b2):\n",
    "    _, _, _, A2 = forward_prop(W0, b1, W1, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def test_prediction(index, W0, b1, W1, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W0, b1, W1, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [9]\n",
      "Label:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [6]\n",
      "Label:  6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(1, W0, b1, W1, b2)\n",
    "test_prediction(2, W0, b1, W1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 8 0 9 8 5 7 8 9 9 1 6 4 9 0 1 7 2 4 9 7 3 4 8 9 9 7 4 7 6 1 1 9 2 1 2\n",
      " 1 1 7 3 3 6 4 8 5 2 2 7 4 7 0 9 9 7 7 0 2 4 7 8 9 1 0 8 9 0 8 1 0 9 9 1 6\n",
      " 2 3 0 7 3 7 6 1 4 7 2 1 6 7 0 9 6 8 3 6 9 9 8 5 5 0 5 8 7 3 4 6 7 1 7 5 1\n",
      " 8 5 9 1 8 4 8 0 9 3 0 4 9 1 6 7 5 9 3 4 5 6 6 3 3 2 2 1 3 7 7 2 4 9 3 6 3\n",
      " 8 8 5 0 4 5 8 3 8 4 6 1 3 4 3 2 8 9 3 4 2 5 7 0 5 6 8 6 0 6 7 3 7 6 6 8 7\n",
      " 3 0 5 0 2 4 9 7 9 4 3 1 8 4 8 7 1 8 7 8 6 6 0 4 7 6 7 7 8 5 0 3 1 1 7 3 3\n",
      " 4 0 4 4 7 9 3 7 0 7 3 3 9 0 2 4 2 3 1 6 7 1 7 3 8 6 8 6 3 1 1 0 1 0 9 6 0\n",
      " 8 7 6 2 5 1 0 2 8 4 9 9 4 8 2 3 6 1 1 6 6 0 9 4 6 1 7 8 9 5 1 7 2 7 1 9 7\n",
      " 5 1 2 4 2 0 4 8 0 0 4 8 4 6 6 7 1 6 0 3 9 3 8 7 7 2 6 0 1 9 2 3 9 5 4 1 5\n",
      " 7 1 7 5 5 7 1 0 0 0 7 0 1 0 1 0 1 4 3 2 7 2 1 8 7 8 8 6 9 4 3 5 2 9 6 1 7\n",
      " 5 0 3 0 0 7 6 1 8 3 2 5 2 8 1 2 1 6 6 9 2 4 1 8 1 1 7 8 0 3 7 8 0 3 2 6 2\n",
      " 8 1 3 0 9 2 1 6 2 2 3 0 3 2 7 9 6 7 5 0 6 6 6 3 2 3 2 1 0 7 1 7 6 0 3 2 7\n",
      " 3 5 1 9 2 7 2 3 1 0 2 9 6 5 9 9 8 2 8 5 1 5 1 3 4 1 6 2 7 3 7 1 6 6 9 5 6\n",
      " 3 5 5 6 7 5 4 3 5 1 3 4 7 0 3 3 6 3 0 8 3 7 6 4 2 6 5 7 7 8 4 3 2 8 0 1 4\n",
      " 0 4 9 0 1 9 9 0 2 2 1 7 0 8 1 9 5 8 8 9 5 9 4 9 3 7 7 0 9 4 3 7 4 0 8 4 6\n",
      " 3 0 3 1 3 0 8 9 8 9 8 9 5 0 7 1 1 3 0 4 1 1 9 7 0 6 9 6 7 7 3 0 4 3 1 2 7\n",
      " 5 4 2 6 3 5 6 0 5 3 2 8 3 4 5 8 4 7 3 6 5 4 7 3 0 5 1 3 5 4 8 3 9 3 2 8 2\n",
      " 1 3 1 7 1 4 3 3 4 1 5 1 9 2 0 5 7 0 1 2 1 5 8 1 1 6 7 6 5 2 3 6 3 5 1 9 7\n",
      " 0 0 4 8 0 6 2 2 8 0 7 2 8 0 9 1 1 3 1 6 2 4 0 3 7 3 0 8 0 7 6 3 3 3 4 5 2\n",
      " 8 6 2 9 6 6 7 3 6 3 4 9 6 6 4 8 9 8 2 4 6 3 6 5 7 1 2 8 3 5 9 1 8 6 4 9 1\n",
      " 6 6 2 1 2 2 4 8 3 0 0 7 0 7 1 6 2 7 6 9 8 3 5 2 2 9 2 2 1 3 9 3 8 9 7 7 3\n",
      " 5 9 1 7 7 4 8 1 2 6 3 7 6 2 8 3 8 2 9 1 1 9 1 0 9 9 3 6 1 3 2 4 8 3 4 0 2\n",
      " 0 9 5 7 4 5 2 4 5 5 8 6 9 1 8 3 4 7 1 3 6 6 7 0 0 6 5 4 3 4 7 2 3 4 5 7 0\n",
      " 5 0 2 5 1 9 4 0 0 9 2 7 6 1 8 1 6 8 1 3 6 7 6 7 3 6 9 3 6 8 1 9 3 5 0 3 7\n",
      " 3 9 2 3 7 3 3 7 4 6 1 1 3 1 0 3 8 1 0 0 1 6 6 5 3 4 6 4 5 2 3 6 8 8 4 0 3\n",
      " 5 6 9 6 1 8 8 9 1 6 1 5 6 3 4 7 4 1 4 6 8 8 7 1 1 3 6 3 5 0 8 8 4 2 0 1 5\n",
      " 8 9 2 9 9 6 4 3 8 6 3 8 9 6 5 5 1 0 4 0 6 6 1 9 9 4 4 5 1 5 0 8 3 8 0 0 3\n",
      " 2] [1 4 8 0 9 8 5 7 8 9 7 1 6 4 9 0 1 7 2 4 9 7 3 4 8 9 9 7 9 7 6 1 1 9 2 4 2\n",
      " 1 1 7 3 3 6 4 1 5 2 2 7 4 7 0 9 9 7 7 0 2 4 7 8 7 1 0 8 9 0 8 1 0 9 9 1 6\n",
      " 2 2 0 7 3 7 6 1 4 4 2 1 6 7 0 9 6 8 3 6 9 9 8 5 5 0 9 8 7 5 4 6 7 1 7 5 1\n",
      " 8 5 9 1 8 9 5 0 9 3 6 4 9 1 6 5 5 9 3 4 5 6 2 8 3 2 2 5 3 1 7 2 4 9 5 6 3\n",
      " 8 3 5 0 4 8 8 3 8 4 6 1 3 4 3 2 5 9 3 4 2 5 9 0 5 6 8 6 4 6 7 3 7 6 6 8 5\n",
      " 3 0 5 0 2 4 9 7 4 4 3 1 5 4 3 7 1 8 7 8 6 6 0 9 9 6 7 7 8 0 0 8 1 1 7 3 2\n",
      " 4 0 4 4 7 9 3 7 0 7 3 3 9 0 3 4 2 8 1 6 7 1 7 3 8 6 8 6 3 1 1 0 1 0 9 6 0\n",
      " 9 9 6 2 5 8 0 2 8 4 9 9 4 4 2 3 6 2 1 5 6 4 4 4 6 1 7 8 9 5 1 7 8 7 1 9 7\n",
      " 5 1 2 4 2 0 4 8 0 0 4 8 4 6 6 3 1 6 2 3 9 3 8 7 7 6 6 0 1 9 2 3 4 5 4 1 5\n",
      " 7 1 7 5 5 7 5 0 0 0 7 0 1 0 1 0 1 4 5 2 7 2 1 8 7 8 8 6 9 4 3 5 2 9 6 1 7\n",
      " 5 0 3 0 0 2 6 1 8 2 5 5 2 8 1 2 1 6 6 9 2 4 1 5 1 1 7 8 0 3 7 8 0 3 2 6 8\n",
      " 8 1 3 7 9 2 1 6 2 2 3 0 3 8 7 3 6 9 5 0 6 6 6 3 2 3 2 1 0 7 1 7 6 0 3 2 7\n",
      " 3 5 1 9 2 7 2 3 1 0 2 9 6 9 9 4 8 2 8 5 1 5 8 3 4 1 6 2 7 3 7 1 6 6 9 5 6\n",
      " 3 5 5 6 3 5 4 3 5 1 3 4 7 2 3 0 6 3 0 8 3 7 6 4 2 6 5 7 7 8 4 3 2 8 6 1 4\n",
      " 0 4 9 0 1 9 9 0 2 2 1 3 0 8 1 9 5 1 8 9 5 9 4 9 3 7 7 0 9 4 3 7 9 0 7 4 6\n",
      " 3 0 8 8 8 4 8 9 8 9 8 9 5 0 7 8 8 3 0 4 5 1 9 3 0 6 9 6 7 7 3 0 4 3 1 4 7\n",
      " 5 4 2 6 3 5 6 0 5 1 2 8 3 4 5 8 4 7 3 6 5 4 7 8 5 8 1 3 5 4 5 3 4 3 2 8 2\n",
      " 1 5 1 7 3 4 3 3 4 1 5 1 9 2 0 5 7 6 1 2 1 5 0 1 1 2 7 5 3 2 3 6 2 5 1 9 7\n",
      " 0 0 4 8 0 6 2 0 3 0 7 2 8 0 9 8 1 3 1 6 2 5 6 3 7 5 0 8 0 7 6 3 3 3 4 5 2\n",
      " 8 6 2 9 6 6 7 3 6 3 4 9 6 6 4 2 8 8 7 4 6 3 6 5 3 7 2 8 3 5 9 4 8 6 4 7 1\n",
      " 6 6 2 1 2 2 9 8 3 0 0 7 0 7 1 6 2 7 6 9 8 3 5 6 2 9 2 2 1 3 9 3 8 9 7 7 2\n",
      " 5 9 1 7 7 4 8 1 2 6 3 7 6 0 8 8 8 2 9 1 1 9 1 0 9 9 3 6 1 1 2 4 8 3 4 0 2\n",
      " 0 9 5 7 4 5 2 6 8 5 8 6 9 1 8 3 4 7 1 3 2 6 7 0 0 2 4 4 3 4 9 2 3 4 5 7 0\n",
      " 5 0 2 8 8 9 6 0 0 9 2 7 4 1 8 1 6 8 1 3 6 7 6 7 3 6 9 3 6 8 8 9 3 5 4 3 7\n",
      " 3 9 2 3 7 9 3 9 4 6 8 1 5 1 0 3 8 1 5 0 1 6 6 5 3 4 6 4 5 2 3 6 5 3 4 0 3\n",
      " 5 6 9 6 1 8 8 8 1 6 1 5 6 3 6 7 4 1 4 6 8 8 7 1 1 5 6 3 5 0 8 8 4 5 0 1 5\n",
      " 1 9 2 9 9 6 4 3 8 8 2 8 9 6 5 8 1 0 5 0 6 6 1 9 9 4 4 5 1 5 0 8 3 8 0 0 3\n",
      " 2]\n",
      "Accuracy for the test set:  0.861\n"
     ]
    }
   ],
   "source": [
    "dev_predictions = make_predictions(X_test, W0, b1, W1, b2)\n",
    "print(\"Accuracy for the test set: \" , get_accuracy(dev_predictions, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network model achieves more than 80% accuracy on both the training and test datasets. This indicates that the model neither overfits nor underfits the data. Instead, it generalizes well, meaning it performs consistently across both seen (training) and unseen (test) data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
